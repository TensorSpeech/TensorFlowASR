{% import "examples/configs/librispeech/sentencepiece/sp.yml.j2" as decoder_config with context %}
{{decoder_config}}

model_config:
  class_name: tensorflow_asr.models.transducer>Conformer
  config:
    name: conformer
    speech_config:
      sample_rate: 16000
      frame_ms: 25
      stride_ms: 10
      num_feature_bins: 80
      feature_type: log_mel_spectrogram
      augmentation_config:
        feature_augment:
          time_masking:
            prob: 1.0
            num_masks: 5
            mask_factor: 100
            p_upperbound: 0.05
          freq_masking:
            prob: 1.0
            num_masks: 1
            mask_factor: 27
    encoder_subsampling:
      type: conv2d
      filters: [144, 144]
      kernels: [3, 3]
      strides: [2, 2]
      paddings: ["causal", "causal"]
      norms: ["none", "none"]
      activations: ["relu", "relu"]
    encoder_ffm_residual_factor: 0.5
    encoder_mhsam_residual_factor: 1.0
    encoder_convm_residual_factor: 1.0
    encoder_dmodel: 144
    encoder_num_blocks: 16
    encoder_head_size: 36 # == dmodel // num_heads
    encoder_num_heads: 4
    encoder_mha_type: relmha
    encoder_interleave_relpe: True
    encoder_use_attention_causal_mask: False
    encoder_use_attention_auto_mask: True
    encoder_kernel_size: 31
    encoder_dropout: 0.1
    encoder_padding: causal
    encoder_memory_length: null
    prediction_label_encode_mode: embedding
    prediction_embed_dim: 320
    prediction_num_rnns: 1
    prediction_rnn_units: 320
    prediction_rnn_type: lstm
    prediction_rnn_implementation: 2
    prediction_rnn_unroll: False
    prediction_layer_norm: False
    prediction_projection_units: 0
    joint_dim: 320
    prejoint_encoder_linear: True
    prejoint_prediction_linear: True
    postjoint_linear: False
    joint_activation: tanh
    joint_mode: add
    blank: 0
    vocab_size: {{decoder_config.vocabsize}}
    kernel_regularizer:
      class_name: l2
      config:
        l2: 1e-6
    bias_regularizer:
      class_name: l2
      config:
        l2: 1e-6

learning_config:
  optimizer_config:
    class_name: Adam
    config:
      learning_rate:
        class_name: tensorflow_asr.optimizers.schedules>TransformerSchedule
        config:
          dmodel: 144
          warmup_steps: 10000
          max_lr: 0.0042
          min_lr: 1e-6
      beta_1: 0.9
      beta_2: 0.98
      epsilon: 1e-9

  gwn_config: null

  gradn_config: null

  running_config:
    batch_size: 2
    num_epochs: 300
    checkpoint:
      filepath: {{modeldir}}/checkpoints/{epoch:02d}.h5
      save_best_only: False
      save_weights_only: True
      save_freq: epoch
    backup_and_restore:
      backup_dir: {{modeldir}}/states
      save_freq: epoch
      delete_checkpoint: False
    tensorboard:
      log_dir: {{modeldir}}/tensorboard
      histogram_freq: 1
      write_graph: False
      write_images: False
      update_freq: epoch
      profile_batch: 100